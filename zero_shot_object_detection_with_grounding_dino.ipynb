{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70bxmZqkKk32"
      },
      "source": [
        "[![Roboflow Notebooks](https://ik.imagekit.io/roboflow/notebooks/template/bannertest2-2.png?ik-sdk-version=javascript-1.4.3&updatedAt=1672932710194)](https://github.com/roboflow/notebooks)\n",
        "\n",
        "# Zero-Shot Object Detection with Grounding DINO\n",
        "\n",
        "---\n",
        "\n",
        "[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/zero-shot-object-detection-with-grounding-dino.ipynb) [![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/IDEA-Research/GroundingDINO) [![arXiv](https://img.shields.io/badge/arXiv-2303.05499-b31b1b.svg)](https://arxiv.org/abs/2303.05499)\n",
        "\n",
        "Grounding DINO can detect **arbitrary objects** with human inputs such as category names or referring expressions. The key solution of open-set object detection is introducing language to a closed-set detector DINO. for open-set concept generalization. If you want to learn more visit official GitHub [repository](https://github.com/IDEA-Research/GroundingDINO) and read the [paper](https://arxiv.org/abs/2303.05499).\n",
        "\n",
        "![grounding dino figure](https://media.roboflow.com/notebooks/examples/grounding-dino-figure.png)\n",
        "\n",
        "## Complementary Materials\n",
        "\n",
        "---\n",
        "\n",
        "[![Roboflow](https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg)](https://blog.roboflow.com/grounding-dino-zero-shot-object-detection) [![YouTube](https://badges.aleen42.com/src/youtube.svg)](https://youtu.be/cMa77r3YrDk)\n",
        "\n",
        "We recommend that you follow along in this notebook while reading the blog post on Grounding DINO. We will talk about the advantages of Grounding DINO, analyze the model architecture, and provide real prompt examples.\n",
        "\n",
        "![grounding dino](https://media.roboflow.com/notebooks/examples/grounding-dino.png)\n",
        "\n",
        "## ‚ö†Ô∏è Disclaimer\n",
        "\n",
        "Grounding DINO codebase is still under development. If you experience any problems with launching the notebook, please let us know and create [issues](https://github.com/roboflow/notebooks/issues) on our GitHub.\n",
        "\n",
        "## Pro Tip: Use GPU Acceleration\n",
        "\n",
        "If you are running this notebook in Google Colab, navigate to `Edit` -> `Notebook settings` -> `Hardware accelerator`, set it to `GPU`, and then click `Save`. This will ensure your notebook uses a GPU, which will significantly speed up model training times.\n",
        "\n",
        "## Steps in this Tutorial\n",
        "\n",
        "In this tutorial, we are going to cover:\n",
        "\n",
        "- Before you start\n",
        "- Install Grounding DINO ü¶ï\n",
        "- Download Grounding DINO Weights üèãÔ∏è\n",
        "- Download Example Data\n",
        "- Load Grounding DINO Model\n",
        "- Grounding DINO Demo\n",
        "- Grounding DINO with Roboflow Dataset\n",
        "- üèÜ Congratulations\n",
        "\n",
        "**Let's begin!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSbs0qbULMk7"
      },
      "source": [
        "## Before you start\n",
        "\n",
        "Let's make sure that we have access to GPU. We can use `nvidia-smi` command to do that. In case of any problems navigate to `Edit` -> `Notebook settings` -> `Hardware accelerator`, set it to `GPU`, and then click `Save`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o2kSrSPdP9YS",
        "outputId": "94231feb-4325-4c36-b8fc-da54a9310864"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pdsIsTKIRxMS",
        "outputId": "21219cb5-149c-4e56-a163-6b5a3f3e2977"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "HOME = os.getcwd()\n",
        "print(HOME)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkrSv4BjMCce"
      },
      "source": [
        "## Install Grounding DINO ü¶ï "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8er1uLGdSQvy",
        "outputId": "d58611ab-55d5-414d-ce1b-b4145d5a378d"
      },
      "outputs": [],
      "source": [
        "# %cd {HOME}\n",
        "# !git clone https://github.com/IDEA-Research/GroundingDINO.git\n",
        "# %cd {HOME}/GroundingDINO\n",
        "# !pip install -q -e .\n",
        "# !pip install -q roboflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nkrkkj7CUlkh",
        "outputId": "3a451a92-8721-4ffe-97b5-840eb2faf9bf"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "CONFIG_PATH = os.path.join(HOME, \"groundingdino/config/GroundingDINO_SwinT_OGC.py\")\n",
        "print(CONFIG_PATH, \"; exist:\", os.path.isfile(CONFIG_PATH))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5XwJupYO4L7"
      },
      "source": [
        "## Download Grounding DINO Weights üèãÔ∏è"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wcYHE6buT_L-",
        "outputId": "440f691b-593a-4d75-c133-7b709c4fb43d"
      },
      "outputs": [],
      "source": [
        "# %cd {HOME}\n",
        "# !mkdir {HOME}/weights\n",
        "# %cd {HOME}/weights\n",
        "\n",
        "# !wget -q https://github.com/IDEA-Research/GroundingDINO/releases/download/v0.1.0-alpha/groundingdino_swint_ogc.pth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3t97tFbEUS3-",
        "outputId": "34749a32-634d-4923-a56b-04180b0a5b41"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "WEIGHTS_NAME = \"groundingdino_swint_ogc.pth\"\n",
        "WEIGHTS_PATH = os.path.join(HOME, \"weights\", WEIGHTS_NAME)\n",
        "print(WEIGHTS_PATH, \"; exist:\", os.path.isfile(WEIGHTS_PATH))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pKiv3-CPaVn"
      },
      "source": [
        "## Download Example Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fWpsrHf6TZPI",
        "outputId": "9a246b32-4e1a-4dc8-e693-af3def8a0289"
      },
      "outputs": [],
      "source": [
        "%cd {HOME}\n",
        "!mkdir {HOME}/data\n",
        "%cd {HOME}/data\n",
        "\n",
        "!wget -q https://media.roboflow.com/notebooks/examples/dog.jpeg\n",
        "!wget -q https://media.roboflow.com/notebooks/examples/dog-2.jpeg\n",
        "!wget -q https://media.roboflow.com/notebooks/examples/dog-3.jpeg\n",
        "!wget -q https://media.roboflow.com/notebooks/examples/dog-4.jpeg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XehYdTlPQkim"
      },
      "source": [
        "## Load Grounding DINO Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57wNyVBgXjTc",
        "outputId": "8f0237f9-394b-489b-f07e-b2e8352ce9f4"
      },
      "outputs": [],
      "source": [
        "# !pip install torchsummary\n",
        "!pip install torchviz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rH62Frfih5lY",
        "outputId": "1dfad462-6379-4de7-ca20-5e165273c74e"
      },
      "outputs": [],
      "source": [
        "# %cd {HOME}/GroundingDINO\n",
        "\n",
        "import torch\n",
        "from groundingdino.util.inference import load_model, load_image, predict, annotate\n",
        "# from torchsummary import summary\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "log_dir = \"./logs\"\n",
        "writer = SummaryWriter(log_dir)\n",
        "\n",
        "model = load_model(CONFIG_PATH, WEIGHTS_PATH)\n",
        "# torch.save(model,'/content/weights/groundingdino_swint_ogc_with_DAG.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0UZneFr3-wMI"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dwhq3RgwQ5Y0"
      },
      "source": [
        "## Grounding DINO Demo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "import torch\n",
        "\n",
        "text_encoder_type = \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(text_encoder_type)\n",
        "\n",
        "# ÂØπÂçïËØç\"chair\"ËøõË°åÁºñÁ†Å\n",
        "encoded = tokenizer(\"chair\")\n",
        "tensor_encoded = torch.tensor(encoded)\n",
        "print(tensor_encoded)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ‰ºÅÂõæÂÆûÁé∞Grounding DINOÁöÑÊ®°ÂûãËÆ°ÁÆóÂõæÂèØËßÜÂåñÔºåÂ∞ùËØïÊú™ÊûúÔºåÂàÜÂà´ËØï‰∫Ütensorboard torchviz networkx netron \n",
        "# from torchsummary import summary\n",
        "import os\n",
        "from torchviz import make_dot\n",
        "from IPython.display import display\n",
        "import pydot\n",
        "\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def preprocess_caption(caption: str) -> str:\n",
        "    result = caption.lower().strip()\n",
        "    if result.endswith(\".\"):\n",
        "        return result\n",
        "    return result + \".\"\n",
        "\n",
        "CONFIG_PATH = os.path.join(HOME, \"groundingdino/config/GroundingDINO_SwinT_OGC.py\")\n",
        "print(CONFIG_PATH, \"; exist:\", os.path.isfile(CONFIG_PATH))\n",
        "\n",
        "WEIGHTS_NAME = \"groundingdino_swint_ogc.pth\"\n",
        "WEIGHTS_PATH = os.path.join(HOME, \"weights\", WEIGHTS_NAME)\n",
        "print(WEIGHTS_PATH, \"; exist:\", os.path.isfile(WEIGHTS_PATH))\n",
        "\n",
        "IMAGE_NAME = \"dog-3.jpeg\"\n",
        "IMAGE_PATH = os.path.join(HOME, \"data\", IMAGE_NAME)\n",
        "\n",
        "TEXT_PROMPT = \"chair\"\n",
        "TEXT_PROMPT = preprocess_caption(TEXT_PROMPT)\n",
        "BOX_TRESHOLD = 0.35\n",
        "TEXT_TRESHOLD = 0.25\n",
        "\n",
        "model = load_model(CONFIG_PATH, WEIGHTS_PATH)\n",
        "\n",
        "image_source, image = load_image(IMAGE_PATH)\n",
        "with torch.no_grad():\n",
        "    outputs = model(image[None], captions=[TEXT_PROMPT])\n",
        "\n",
        "# torch.onnx.export(model, (image[None], captions=[TEXT_PROMPT]), './groundingdino_swint_ogc_with_DAG_0606_1.onnx')\n",
        "torch.onnx.export(model, (image[None],), './groundingdino_swint_ogc_with_DAG_0606_1.on',  captions=(TEXT_PROMPT,))\n",
        "torch.save(model, './groundingdino_swint_ogc_with_DAG_0606_1.pth')\n",
        "\n",
        "# torch.save(model,'./groundingdino_swint_ogc_with_DAG_0606.pth')\n",
        "\n",
        "# summary(model, (image[None], captions=[TEXT_PROMPT]))\n",
        "# dot = make_dot(outputs[\"pred_logits\"])\n",
        "# G = nx.DiGraph(dot)\n",
        "# pos = nx.nx_pydot.graphviz_layout(G, prog='dot')\n",
        "# nx.draw(G, pos, with_labels=True, arrows=False)\n",
        "# plt.show()\n",
        "\n",
        "# dot = make_dot(outputs)\n",
        "# pydot_graph = pydot.graph_from_dot_data(dot.source)[0]\n",
        "# pydot_graph.write_png('graph.png')\n",
        "\n",
        "# outputs = {k: str(v.data_ptr()) if isinstance(v, torch.Tensor) else v for k, v in outputs.items()}\n",
        "# outputs = {k: v.id if isinstance(v, torch.Tensor) else v for k, v in outputs.items()}\n",
        "# dot = make_dot(outputs, params=None)\n",
        "# dot = make_dot(outputs, params=dict(model.named_parameters()))\n",
        "# dot = make_dot(outputs[\"pred_logits\"])\n",
        "# display(dot)\n",
        "# log_dir = \"./logs\"\n",
        "# writer = SummaryWriter(log_dir)\n",
        "# writer.add_graph(model)\n",
        "# writer.add_graph(model, input_to_model={\"samples\": image[None], \"captions\": TEXT_PROMPT})\n",
        "# writer.add_graph(model,(image[None], [TEXT_PROMPT]))\n",
        "\n",
        "# # Â∞ÜÊ®°ÂûãËÆ°ÁÆóÂõæÂÜôÂÖ•Êó•ÂøóÊñá‰ª∂\n",
        "# inputs = (image,TEXT_PROMPT)\n",
        "# dot = make_dot(model(*inputs), params=dict(model.named_parameters()))\n",
        "# writer.add_graph(dot)\n",
        "\n",
        "# # ÂÖ≥Èó≠ SummaryWriter ÂØπË±°\n",
        "# writer.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "def preprocess_caption(caption: str) -> str:\n",
        "    result = caption.lower().strip()\n",
        "    if result.endswith(\".\"):\n",
        "        return result\n",
        "    return result + \".\"\n",
        "\n",
        "CONFIG_PATH = os.path.join(HOME, \"groundingdino/config/GroundingDINO_SwinT_OGC.py\")\n",
        "print(CONFIG_PATH, \"; exist:\", os.path.isfile(CONFIG_PATH))\n",
        "\n",
        "WEIGHTS_NAME = \"groundingdino_swint_ogc.pth\"\n",
        "WEIGHTS_PATH = os.path.join(HOME, \"weights\", WEIGHTS_NAME)\n",
        "print(WEIGHTS_PATH, \"; exist:\", os.path.isfile(WEIGHTS_PATH))\n",
        "\n",
        "IMAGE_NAME = \"dog-3.jpeg\"\n",
        "IMAGE_PATH = os.path.join(HOME, \"data\", IMAGE_NAME)\n",
        "\n",
        "TEXT_PROMPT = \"chair\"\n",
        "TEXT_PROMPT = preprocess_caption(TEXT_PROMPT)\n",
        "BOX_TRESHOLD = 0.35\n",
        "TEXT_TRESHOLD = 0.25\n",
        "\n",
        "model = load_model(CONFIG_PATH, WEIGHTS_PATH)\n",
        "\n",
        "image_source, image = load_image(IMAGE_PATH)\n",
        "with torch.no_grad():\n",
        "    outputs = model(image[None], captions=[TEXT_PROMPT])\n",
        "\n",
        "log_dir = \"./logs\"\n",
        "writer = SummaryWriter(log_dir)\n",
        "# writer.add_graph(model, input_to_model={\"samples\": image[None], \"captions\": TEXT_PROMPT})\n",
        "writer.add_graph(model,(image[None], [TEXT_PROMPT]))\n",
        "writer.close()\n",
        "\n",
        "\n",
        "# outputs = {k: str(v.data_ptr()) if isinstance(v, torch.Tensor) else v for k, v in outputs.items()}\n",
        "# outputs = {k: v.id if isinstance(v, torch.Tensor) else v for k, v in outputs.items()}\n",
        "# dot = make_dot(outputs, params=None)\n",
        "# dot = make_dot(outputs, params=dict(model.named_parameters()))\n",
        "# dot = make_dot(outputs[\"pred_logits\"])\n",
        "# display(dot)\n",
        "\n",
        "# writer.add_graph(model)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 720
        },
        "id": "psf2DLyLlXvs",
        "outputId": "a3b6bbdc-b1d2-4038-ccca-8eddb4e15f5c"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import supervision as sv\n",
        "\n",
        "# from torch.utils.tensorboard import SummaryWriter\n",
        "# from torchviz import make_dot\n",
        "\n",
        "IMAGE_NAME = \"dog-3.jpeg\"\n",
        "IMAGE_PATH = os.path.join(HOME, \"data\", IMAGE_NAME)\n",
        "\n",
        "TEXT_PROMPT = \"chair\"\n",
        "BOX_TRESHOLD = 0.35\n",
        "TEXT_TRESHOLD = 0.25\n",
        "\n",
        "text_exaple = torch.randn(1)\n",
        "\n",
        "image_source, image = load_image(IMAGE_PATH)\n",
        "\n",
        "boxes, logits, phrases = predict(\n",
        "    model=model, \n",
        "    image=image, \n",
        "    caption=TEXT_PROMPT, \n",
        "    box_threshold=BOX_TRESHOLD, \n",
        "    text_threshold=TEXT_TRESHOLD\n",
        ")\n",
        "\n",
        "annotated_frame = annotate(image_source=image_source, boxes=boxes, logits=logits, phrases=phrases)\n",
        "\n",
        "\n",
        "\n",
        "# print(model)\n",
        "%matplotlib inline  \n",
        "sv.plot_image(annotated_frame, (16, 16))\n",
        "# sv.plot_image(torch.randn(20,20),size=(16, 16))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# ÊâìÂç∞Ê®°ÂûãÊØè‰∏ÄÂ±ÇÁöÑ‰ø°ÊÅØÂèäËæìÂá∫\n",
        "for i, layer in enumerate(model.modules()):\n",
        "    print(f\"Layer {i}: {layer.__class__.__name__}\")\n",
        "    # print(f\"Input shape: {layer.in_features}\")\n",
        "    # print(f\"Output shape: {layer.out_features}\")\n",
        "    print()\n",
        "\n",
        "# ÊâìÂç∞Ê®°ÂûãÊØè‰∏ÄÂ±ÇÁöÑÁâπÂæÅËæìÂá∫\n",
        "# with torch.no_grad():\n",
        "#     for layer in model.modules():\n",
        "#         intermediate_layer_model = torch.nn.Sequential(*list(model.children())[:list(model.modules()).index(layer)+1])\n",
        "#         intermediate_output = intermediate_layer_model(input_data)\n",
        "#         print(f\"Layer {layer.__class__.__name__} output shape: {intermediate_output.shape}\")\n",
        "#         print(f\"Layer {layer.__class__.__name__} output: {intermediate_output}\")\n",
        "#         print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ËØ•Ê®°ÂùóÊâìÂç∞‰∫ÜÁ®ãÂ∫èËøêË°åËøáÁ®ã‰∏≠Âä†ËΩΩÊ®°ÂûãÊé®ÁêÜÁöÑÂ±ÇÁöÑËæìÂá∫‰ø°ÊÅØÔºåÁõÆÂâçÊù•ÁúãÊØîËæÉÊúâÁî®ÁöÑ‰∏Ä‰∏™Âú∞Êñπ\n",
        "import torch\n",
        "\n",
        "# ÂÆö‰πâ‰∏Ä‰∏™Èí©Â≠êÂáΩÊï∞\n",
        "def print_output_shape(module, input, module_output):\n",
        "    # print(module)\n",
        "    print(f\"The module of GDINO: {module.__class__.__name__}\")\n",
        "    if isinstance(module_output,tuple):\n",
        "        print(f\"The module output: {module_output.__class__.__name__}\")\n",
        "        for i, output in enumerate(module_output):\n",
        "            if isinstance(output, torch.Tensor):\n",
        "                print(f\"{output} shape: {output.shape}\")\n",
        "            else:\n",
        "                print(f\"Unknown output type: {type(output)}\")\n",
        "    else:\n",
        "        if isinstance(module_output, torch.Tensor):\n",
        "            print(f\"{module_output} shape: {module_output.shape}\")\n",
        "        else:\n",
        "            print(f\"Unknown output type: {type(module_output)}\")\n",
        "            # print(type(module_output))\n",
        "            print(len(module_output))\n",
        "        # print(module_output.shape)\n",
        "        # pass\n",
        "\n",
        "\n",
        "# Ê≥®ÂÜåÈí©Â≠êÂáΩÊï∞ÔºåÂØπÊâÄÊúâÊ®°ÂùóÈÉΩÁîüÊïà\n",
        "handles = []\n",
        "for module in model.modules():\n",
        "    handle = module.register_forward_hook(print_output_shape)\n",
        "    handles.append(handle)\n",
        "print(len(handles))\n",
        "# ÂâçÂêëÊé®ÁêÜ\n",
        "boxes, logits, phrases = predict(\n",
        "    model=model, \n",
        "    image=image, \n",
        "    caption=TEXT_PROMPT, \n",
        "    box_threshold=BOX_TRESHOLD, \n",
        "    text_threshold=TEXT_TRESHOLD\n",
        ")\n",
        "\n",
        "# ÁßªÈô§ÊâÄÊúâÈí©Â≠êÂáΩÊï∞\n",
        "for handle in handles:\n",
        "    handle.remove()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from transformers import BertTokenizer\n",
        "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "# token_ids = tokenizer.convert_tokens_to_ids([\"[CLS]\", \"[SEP]\", \".\", \"?\"])\n",
        "# print(token_ids)\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "text = \"This is a sentence.\"\n",
        "# tokens = tokenizer.tokenize(text)\n",
        "token_ids = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text))\n",
        "print(token_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "VKzXm8mNR2XS",
        "outputId": "aeda67c6-8474-4809-d2dc-73759fe73fd5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import supervision as sv\n",
        "\n",
        "# IMAGE_NAME = \"dog-3.jpeg\"\n",
        "IMAGE_NAME = \"/data/ML_document/datasets/custom_6dpose_dataset/train/5.png\"\n",
        "IMAGE_PATH = os.path.join(HOME, \"data\", IMAGE_NAME)\n",
        "\n",
        "TEXT_PROMPT = \"chair with man sitting on it\"\n",
        "BOX_TRESHOLD = 0.35\n",
        "TEXT_TRESHOLD = 0.25\n",
        "\n",
        "image_source, image = load_image(IMAGE_PATH)\n",
        "\n",
        "boxes, logits, phrases = predict(\n",
        "    model=model, \n",
        "    image=image, \n",
        "    caption=TEXT_PROMPT, \n",
        "    box_threshold=BOX_TRESHOLD, \n",
        "    text_threshold=TEXT_TRESHOLD\n",
        ")\n",
        "\n",
        "annotated_frame = annotate(image_source=image_source, boxes=boxes, logits=logits, phrases=phrases)\n",
        "\n",
        "%matplotlib inline  \n",
        "sv.plot_image(annotated_frame, (16, 16))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "B-bgwhzfTFKh",
        "outputId": "ddf1741a-41c2-42e6-d48b-6fa28580218d"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import supervision as sv\n",
        "\n",
        "IMAGE_NAME = \"dog-3.jpeg\"\n",
        "IMAGE_PATH = os.path.join(HOME, \"data\", IMAGE_NAME)\n",
        "\n",
        "TEXT_PROMPT = \"chair, dog, table, shoe, light bulb, coffee, hat, glasses, car, tail, umbrella\"\n",
        "BOX_TRESHOLD = 0.35\n",
        "TEXT_TRESHOLD = 0.25\n",
        "\n",
        "image_source, image = load_image(IMAGE_PATH)\n",
        "\n",
        "boxes, logits, phrases = predict(\n",
        "    model=model, \n",
        "    image=image, \n",
        "    caption=TEXT_PROMPT, \n",
        "    box_threshold=BOX_TRESHOLD, \n",
        "    text_threshold=TEXT_TRESHOLD\n",
        ")\n",
        "\n",
        "annotated_frame = annotate(image_source=image_source, boxes=boxes, logits=logits, phrases=phrases)\n",
        "\n",
        "%matplotlib inline  \n",
        "sv.plot_image(annotated_frame, (16, 16))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "zcQc8EeOSXKs",
        "outputId": "ba0f3a39-ca87-43d5-db79-dd7f420034bf"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import supervision as sv\n",
        "\n",
        "IMAGE_NAME = \"dog-2.jpeg\"\n",
        "IMAGE_PATH = os.path.join(HOME, \"data\", IMAGE_NAME)\n",
        "\n",
        "TEXT_PROMPT = \"glass\"\n",
        "BOX_TRESHOLD = 0.35\n",
        "TEXT_TRESHOLD = 0.25\n",
        "\n",
        "image_source, image = load_image(IMAGE_PATH)\n",
        "\n",
        "boxes, logits, phrases = predict(\n",
        "    model=model, \n",
        "    image=image, \n",
        "    caption=TEXT_PROMPT, \n",
        "    box_threshold=BOX_TRESHOLD, \n",
        "    text_threshold=TEXT_TRESHOLD\n",
        ")\n",
        "\n",
        "annotated_frame = annotate(image_source=image_source, boxes=boxes, logits=logits, phrases=phrases)\n",
        "\n",
        "%matplotlib inline  \n",
        "sv.plot_image(annotated_frame, (16, 16))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ys7nAPdqSs2i",
        "outputId": "e50295ad-4bd5-4c44-e9e4-1bd091d90d15"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import supervision as sv\n",
        "\n",
        "IMAGE_NAME = \"dog-2.jpeg\"\n",
        "IMAGE_PATH = os.path.join(HOME, \"data\", IMAGE_NAME)\n",
        "\n",
        "TEXT_PROMPT = \"glass most to the right\"\n",
        "BOX_TRESHOLD = 0.35\n",
        "TEXT_TRESHOLD = 0.25\n",
        "\n",
        "image_source, image = load_image(IMAGE_PATH)\n",
        "\n",
        "boxes, logits, phrases = predict(\n",
        "    model=model, \n",
        "    image=image, \n",
        "    caption=TEXT_PROMPT, \n",
        "    box_threshold=BOX_TRESHOLD, \n",
        "    text_threshold=TEXT_TRESHOLD\n",
        ")\n",
        "\n",
        "annotated_frame = annotate(image_source=image_source, boxes=boxes, logits=logits, phrases=phrases)\n",
        "\n",
        "%matplotlib inline  \n",
        "sv.plot_image(annotated_frame, (16, 16))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "8MGSwH8TSl9G",
        "outputId": "33d5694a-ddd4-4adc-b1b4-ef9b2fdb4b22"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import supervision as sv\n",
        "\n",
        "IMAGE_NAME = \"dog-2.jpeg\"\n",
        "IMAGE_PATH = os.path.join(HOME, \"data\", IMAGE_NAME)\n",
        "\n",
        "TEXT_PROMPT = \"straw\"\n",
        "BOX_TRESHOLD = 0.35\n",
        "TEXT_TRESHOLD = 0.25\n",
        "\n",
        "image_source, image = load_image(IMAGE_PATH)\n",
        "\n",
        "boxes, logits, phrases = predict(\n",
        "    model=model, \n",
        "    image=image, \n",
        "    caption=TEXT_PROMPT, \n",
        "    box_threshold=BOX_TRESHOLD, \n",
        "    text_threshold=TEXT_TRESHOLD\n",
        ")\n",
        "\n",
        "annotated_frame = annotate(image_source=image_source, boxes=boxes, logits=logits, phrases=phrases)\n",
        "\n",
        "%matplotlib inline  \n",
        "sv.plot_image(annotated_frame, (16, 16))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "YLifwwh6S718",
        "outputId": "ffd5ce5f-ba3d-480d-a965-85e6a8ecceab"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import supervision as sv\n",
        "\n",
        "IMAGE_NAME = \"dog-4.jpeg\"\n",
        "IMAGE_PATH = os.path.join(HOME, \"data\", IMAGE_NAME)\n",
        "\n",
        "TEXT_PROMPT = \"mens shadow\"\n",
        "BOX_TRESHOLD = 0.35\n",
        "TEXT_TRESHOLD = 0.25\n",
        "\n",
        "image_source, image = load_image(IMAGE_PATH)\n",
        "\n",
        "boxes, logits, phrases = predict(\n",
        "    model=model, \n",
        "    image=image, \n",
        "    caption=TEXT_PROMPT, \n",
        "    box_threshold=BOX_TRESHOLD, \n",
        "    text_threshold=TEXT_TRESHOLD\n",
        ")\n",
        "\n",
        "annotated_frame = annotate(image_source=image_source, boxes=boxes, logits=logits, phrases=phrases)\n",
        "\n",
        "%matplotlib inline  \n",
        "sv.plot_image(annotated_frame, (16, 16))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_O2brG-4XIJl"
      },
      "source": [
        "## Grounding DINO with Roboflow Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1rgJrqOiaqrU",
        "outputId": "98133dea-6676-4bef-f3ce-07687ccaa7ee"
      },
      "outputs": [],
      "source": [
        "%cd {HOME}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "suRStzQ2YIrE",
        "outputId": "b2b206bb-f10f-4184-abeb-b898be38492f"
      },
      "outputs": [],
      "source": [
        "import roboflow\n",
        "\n",
        "roboflow.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v3WvxHWJc77j"
      },
      "outputs": [],
      "source": [
        "from random import randrange\n",
        "from roboflow.core.dataset import Dataset\n",
        "\n",
        "\n",
        "def pick_random_image(dataset: Dataset, subdirrectory: str = \"valid\") -> str:\n",
        "    image_directory_path = f\"{dataset.location}/{subdirrectory}\"\n",
        "    image_names = os.listdir(image_directory_path)\n",
        "    image_index = randrange(len(image_names))\n",
        "    image_name = image_names[image_index]\n",
        "    image_path = os.path.join(image_directory_path, image_name)\n",
        "    return image_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D_FHahEGY8NV",
        "outputId": "80f13e44-0d56-4077-f81d-be7bf1114ead"
      },
      "outputs": [],
      "source": [
        "from roboflow import Roboflow\n",
        "rf = Roboflow()\n",
        "project = rf.workspace(\"work-safe-project\").project(\"safety-vest---v4\")\n",
        "dataset = project.version(3).download(\"coco\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "IMe-bP1vV7Pj",
        "outputId": "afdd8bc0-3ed2-4bb7-ae6d-db04798b02ef"
      },
      "outputs": [],
      "source": [
        "TEXT_PROMPT = \", \".join(project.classes.keys())\n",
        "TEXT_PROMPT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rcTCSjWAMtN0"
      },
      "outputs": [],
      "source": [
        "image_path = pick_random_image(dataset=dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "QE9aSXo9e1jR",
        "outputId": "5e58b59b-bb92-491c-f0ec-c84d92a6c0d1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import supervision as sv\n",
        "\n",
        "BOX_TRESHOLD = 0.35\n",
        "TEXT_TRESHOLD = 0.25\n",
        "\n",
        "image_source, image = load_image(image_path)\n",
        "\n",
        "boxes, logits, phrases = predict(\n",
        "    model=model, \n",
        "    image=image, \n",
        "    caption=TEXT_PROMPT, \n",
        "    box_threshold=BOX_TRESHOLD, \n",
        "    text_threshold=TEXT_TRESHOLD\n",
        ")\n",
        "\n",
        "annotated_frame = annotate(image_source=image_source, boxes=boxes, logits=logits, phrases=phrases)\n",
        "\n",
        "%matplotlib inline  \n",
        "sv.plot_image(annotated_frame, (16, 16))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNrvcDzANRli"
      },
      "source": [
        "**NOTE:** The design of the prompt is very important. Try to be as accurate as possible. Avoid abbreviations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kWSe3bEOM_gT"
      },
      "outputs": [],
      "source": [
        "TEXT_PROMPT = \"reflective safety vest, helmet, head, nonreflective safety vest\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "yY-sYGESNGGb",
        "outputId": "80eed1aa-e922-4619-87be-c745ab929c47"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import supervision as sv\n",
        "\n",
        "BOX_TRESHOLD = 0.35\n",
        "TEXT_TRESHOLD = 0.25\n",
        "\n",
        "image_source, image = load_image(image_path)\n",
        "\n",
        "boxes, logits, phrases = predict(\n",
        "    model=model, \n",
        "    image=image, \n",
        "    caption=TEXT_PROMPT, \n",
        "    box_threshold=BOX_TRESHOLD, \n",
        "    text_threshold=TEXT_TRESHOLD\n",
        ")\n",
        "\n",
        "annotated_frame = annotate(image_source=image_source, boxes=boxes, logits=logits, phrases=phrases)\n",
        "\n",
        "%matplotlib inline  \n",
        "sv.plot_image(annotated_frame, (16, 16))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmubTJSATsm-"
      },
      "source": [
        "## üèÜ Congratulations\n",
        "\n",
        "### Learning Resources\n",
        "\n",
        "Roboflow has produced many resources that you may find interesting as you advance your knowledge of computer vision:\n",
        "\n",
        "- [Roboflow Notebooks](https://github.com/roboflow/notebooks): A repository of over 20 notebooks that walk through how to train custom models with a range of model types, from YOLOv7 to SegFormer.\n",
        "- [Roboflow YouTube](https://www.youtube.com/c/Roboflow): Our library of videos featuring deep dives into the latest in computer vision, detailed tutorials that accompany our notebooks, and more.\n",
        "- [Roboflow Discuss](https://discuss.roboflow.com/): Have a question about how to do something on Roboflow? Ask your question on our discussion forum.\n",
        "- [Roboflow Models](https://roboflow.com): Learn about state-of-the-art models and their performance. Find links and tutorials to guide your learning.\n",
        "\n",
        "### Convert data formats\n",
        "\n",
        "Roboflow provides free utilities to convert data between dozens of popular computer vision formats. Check out [Roboflow Formats](https://roboflow.com/formats) to find tutorials on how to convert data between formats in a few clicks.\n",
        "\n",
        "### Connect computer vision to your project logic\n",
        "\n",
        "[Roboflow Templates](https://roboflow.com/templates) is a public gallery of code snippets that you can use to connect computer vision to your project logic. Code snippets range from sending emails after inference to measuring object distance between detections."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.8.16",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    },
    "vscode": {
      "interpreter": {
        "hash": "22dba8a3c1b3932e6b19a72b0abaa3c7d0db1b9c78e9c83543146071bd8b0aa6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
